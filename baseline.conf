dataset = ${dataset_raw}

dataset_raw = {
    dir = ../../datasets/kaggle/quora
    train = train.csv
    test = test.csv
    fillna = nan
}

dataset_cleaned = {
    dir = dumps2/cleaning_00_as
    train = train.csv
    test = test.csv
    fillna = na
}

cleaning = {
    dump = {
        dir = dumps/cleaning_00_as
    }

    remove_stopwords = false
    stem_words = false
}

weights = [
    {
        class = 1
        weight = 0.472 # 0.513542
    },
    {
        class = 0
        weight = 1.309 # 1.2847
    }
]

cv = {
    nfolds = 10
    seed = 98720
}

linear = {

    dataset = ${dataset_raw}

    word_diff = {
        enabled = true

        dump = {
            dir = dumps/linear_word_diff_as_00

            cache = {
                enabled = true
                train = features_train.npz
                test = features_test.npz
            }
        }

        vectorizer = {
            analyzer = word
            ngram_range = [1, 3]
            min_df = 1
        }

        combine = diff

        model = {
            solver = lbfgs
            max_iter = 250
            penalty = l2
            alpha = 1.1
            seed=287734
        }
    }

    word_intersect = {
        enabled = true

        dump = {
            dir = dumps/linear_word_intersect_as_00

            cache = {
                enabled = true
                train = features_train.npz
                test = features_test.npz
            }
        }

        vectorizer = {
            analyzer = word
            ngram_range = [1, 3]
            min_df = 1
        }

        combine = intersect

        model = {
            solver = lbfgs
            max_iter = 250
            penalty = l2
            alpha = 1.0
            seed=287734
        }
    }


    char_diff = {
        enabled = true

        dump = {
            dir = dumps/linear_char_diff_as_00

            cache = {
                enabled = true
                train = features_train.npz
                test =  features_test.npz
            }
        }

        vectorizer = {
            analyzer = char
            ngram_range = [1, 7]
            min_df = 5
        }

        combine = diff

        model = {
            solver = lbfgs
            max_iter = 250
            penalty = l2
            alpha = 0.05
            seed=287734
        }
    }

    char_intersect = {
        enabled = true

        dump = {
            dir = dumps/linear_char_intersect_as_00

            cache = {
                enabled = true
                train = features_train.npz
                test =  features_test.npz
            }
        }

        vectorizer = {
            analyzer = char
            ngram_range = [1, 7]
            min_df = 5
        }

        combine = intersect

        model = {
            solver = lbfgs
            max_iter = 250
            penalty = l2
            alpha = 0.1
            seed=287734
        }
    }
}

counters = {
    dataset = ${dataset_raw}

    dump = {
        dir = dumps/counters_as_04
    }
}

simplest = {
    dataset = ${dataset_raw}

    dump = {
        dir = dumps/simplest_as_00
    }
}

tfidf = {
    dataset = ${dataset_raw}

    dump = {
        dir = dumps/tfidf_as_00
    }
}

distances = {
    dataset = ${dataset_raw}

    dump = {
        dir = dumps/distances_as_00
    }
}

fuzzy = {
    dataset = ${dataset_raw}

    dump = {
        dir = dumps/fuzzy_as_00
    }
}

svd = {
    dataset = ${dataset_raw}

    svd_word_diff = {
        enabled = false

        dump = {
            dir = dumps/svd_word_diff_as_00
        }

        vectorizer = {
            analyzer = word
            ngram_range = [1, 3]
            min_df = 1
            binary = True
        }

        model = {
            transform = diff
            method = ARPACK
            k = 10
            maxiter = 100
            tol = 1.e-4
        }

    }

    svd_word_intersect = {
        enabled = false

        dump = {
            dir = dumps/svd_word_intersect_as_00
        }

        vectorizer = {
            analyzer = word
            ngram_range = [1, 3]
            min_df = 1
            binary = True
        }

        model = {
            transform = intersect
            method = ARPACK
            k = 10
            maxiter = 100
            tol = 1.e-4
        }

    }

    svd_word_stack = {
        enabled = false

        dump = {
            dir = dumps/svd_word_stack_as_00
        }

        vectorizer = {
            analyzer = word
            ngram_range = [1, 3]
            min_df = 1
            binary = True
        }

        model = {
            transform = stack
            method = ARPACK
            k = 10
            maxiter = 100
            tol = 1.e-4
        }

    }

    svd_char_diff = {
        enabled = false

        dump = {
            dir = dumps/svd_char_diff_as_00
        }

        vectorizer = {
            analyzer = word
            ngram_range = [1, 7]
            min_df = 5
            binary = True
        }

        model = {
            transform = diff
            method = ARPACK
            k = 10
            maxiter = 100
            tol = 1.e-4
        }
    }

    svd_char_intersect = {
        enabled = false

        dump = {
            dir = dumps/svd_char_intersect_as_00
        }

        vectorizer = {
            analyzer = word
            ngram_range = [1, 7]
            min_df = 5
            binary = True
        }

        model = {
            transform = intersect
            method = ARPACK
            k = 10
            maxiter = 100
            tol = 1.e-4
        }
    }

    svd_char_stack = {
        enabled = true

        dump = {
            dir = dumps/svd_char_stack_as_00
        }

        vectorizer = {
            analyzer = char
            ngram_range = [1, 3]
            min_df = 1
            binary = True
        }

        model = {
            transform = stack
            method = ARPACK
            k = 10
            maxiter = 100
            tol = 1.e-4
        }

    }

}

svdff = {
    dataset = ${dataset_raw}

    dump = {
        dir = dumps/svdff_as_00
    }

    vectorizer = {
        analyzer = char
        ngram_range = [1, 7]
        min_df = 10
        binary = True
    }

    svd = {
        transform = stack
        k = 100
        maxiter = 100
        method = ARPACK
        tol = 1.e-4
    }

    ff = {
        layers = [30]
        activations = [relu]
        method = adam
        epochs = 50
        batch_size = 1000
    }

}

svdxgb = {
    dataset = ${dataset_raw}

    dump = {
        dir = dumps/svdxgb_as_00
    }

    vectorizer = {
        analyzer = char
        ngram_range = [1, 7]
        min_df = 10
        binary = True
    }

    svd = {
        transform = stack
        k = 100
        maxiter = 100
        method = ARPACK
        tol = 1.e-4
    }

    model = {
        num_round = 1000
        max_depth = 6
        objective = binary:logistic
        nthread = 4
        eta = 0.02
        eval_metric = [logloss, auc]
    }
}

word2vec = {
    enabled = true

    dataset = ${dataset_cleaned}

    dump = {
        dir = dumps/word2vec_as_00
    }

    embeddings = {
        dir = ../../datasets/kaggle/quora/word2vec
        file = GoogleNews-vectors-negative300.bin.gz
    }
}

glove = {
    enabled = true

    dataset = ${dataset_cleaned}

    dump = {
        dir = dumps/glove_as_00
    }

    embeddings = {
        dir = ../../datasets/kaggle/quora/glove
        file = glove.840B.300d.converted.txt
    }
}

exploration = {
    dump = {
        dir = dumps/features_set_01_as_00
        notebook = exploration.ipynb
        images = {
            dir = images
        }
    }
}

features = {
    linear_word_diff = {
        dump = dumps/linear_word_diff_as_00
        features = [
            {
                feature = linear_word_diff
                train_col = linear
                test_col = linear_cv
            }
        ]
    }

    linear_word_intersect = {
        dump = dumps/linear_word_intersect_as_00
        features = [
            {
                feature = linear_word_intersect
                train_col = linear
                test_col = linear_cv
            }
        ]
    }

    linear_char_diff = {
        dump = dumps/linear_char_diff_as_00
        features = [
            {
                feature = linear_char_diff
                train_col = linear
                test_col = linear_cv
            }
        ]
    }

    linear_char_intersect = {
        dump = dumps/linear_char_intersect_as_00
        features = [
            {
                feature = linear_char_intersect
                train_col = linear
                test_col = linear_cv
            }
        ]
    }

    counters = {
        dump = dumps/counters_as_04
        features = [
            {
                feature = freq_q1
            },
            {
                feature = freq_q2
            },
            {
                feature = intersect_q1_q2
            },
            {
                feature = intersect2_q1_q2
            }
        ]
    }

    simplest = {
        dump = dumps/simplest_as_00
        features = [
            {
                feature = len_q1
            },
            {
                feature = len_q2
            },
            {
                feature = diff_len
            },
            {
                feature = len_word_q1
            },
            {
                feature = len_word_q2
            },
            {
                feature = diff_len_word
            },
            {
                feature = len_char_q1
            },
            {
                feature = len_char_q2
            },
            {
                feature = diff_len_char
            }
        ]
    }

    tfidf = {
        dump = dumps/tfidf_as_00
        features = [
            {
                feature = tfidf_wm
            },
            {
                feature = tfidf_wm_stops
            }
        ]
    }

    distances = {
        dump = dumps/distances_as_00
        features = [
            {
                feature = jaccard
            },
            {
                feature = levenstein1
            },
            {
                feature = levenstein2
            },
            {
                feature = sorensen
            }
        ]
    }

    fuzzy = {
        dump = dumps/fuzzy_as_00
        features = [
            {
                feature = qratio
            },
            {
                feature = wratio
            },
            {
                feature = partial_ratio
            },
            {
                feature = partial_token_set_ratio
            },
            {
                feature = partial_token_sort_ratio
            },
            {
                feature = token_set_ratio
            },
            {
                feature = token_sort_ratio
            }
        ]
    }

    svd_word_diff = {
        dump = dumps/svd_word_diff_as_00

        features = [
            {
                feature = svd_word_diff_7
            },
            {
                feature = svd_word_diff_8
            },
            {
                feature = svd_word_diff_9
            }
        ]
    }

    svd_word_intersect = {
        dump = dumps/svd_word_intersect_as_00

        features = [
            {
                feature = svd_word_intersect_7
            },
            {
                feature = svd_word_intersect_8
            },
            {
                feature = svd_word_intersect_9
            }
        ]
    }

    svd_char_diff = {
        dump = dumps/svd_char_diff_as_00

        features = [
            {
                feature = svd_char_diff_7
            },
            {
                feature = svd_char_diff_8
            },
            {
                feature = svd_char_diff_9
            }
        ]
    }

    svd_char_intersect = {
        dump = dumps/svd_char_intersect_as_00

        features = [
            {
                feature = svd_char_intersect_7
            },
            {
                feature = svd_char_intersect_8
            },
            {
                feature = svd_char_intersect_9
            }
        ]
    }

    svd_char_stack {
        dump = "dumps/svd_char_stack_as_00"
        features = [
            {
                feature = "svd_char_stack_0_q1"
            }
            {
                feature = "svd_char_stack_0_q2"
            }
            {
                feature = "svd_char_stack_1_q1"
            }
            {
                feature = "svd_char_stack_1_q2"
            }
            {
                feature = "svd_char_stack_2_q1"
            }
            {
                feature = "svd_char_stack_2_q2"
            }
            {
                feature = "svd_char_stack_3_q1"
            }
            {
                feature = "svd_char_stack_3_q2"
            }
            {
                feature = "svd_char_stack_4_q1"
            }
            {
                feature = "svd_char_stack_4_q2"
            }
            {
                feature = "svd_char_stack_5_q1"
            }
            {
                feature = "svd_char_stack_5_q2"
            }
            {
                feature = "svd_char_stack_6_q1"
            }
            {
                feature = "svd_char_stack_6_q2"
            }
            {
                feature = "svd_char_stack_7_q1"
            }
            {
                feature = "svd_char_stack_7_q2"
            }
            {
                feature = "svd_char_stack_8_q1"
            }
            {
                feature = "svd_char_stack_8_q2"
            }
            {
                feature = "svd_char_stack_9_q1"
            }
            {
                feature = "svd_char_stack_9_q2"
            }
        ]
    }

    word2vec = {
        dump = dumps2/word2vec_as_00

        features = [
            {
                feature = w2v_wmd_norm
            },
            {
                feature = w2v_skew_q1
            },
            {
                feature = w2v_skew_q2
            },
            {
                feature = w2v_kurt_q1
            },
            {
                feature = w2v_kurt_q2
            }
        ]
    }

    glove = {
        dump = dumps/glove_as_00

        features = [
            {
                feature = glove_wmd
            },
            {
                feature = glove_cos
            },
            {
                feature = glove_city
            },
            {
                feature = glove_eucl
            },
            {
                feature = glove_mink
            },
            {
                feature = glove_skew_q1
            },
            {
                feature = glove_skew_q2
            },
            {
                feature = glove_kurt_q1
            },
            {
                feature = glove_kurt_q2
            }
        ]
    }

    fuzzy = {
        dump = dumps/fuzzy_as_00

        feature = [
            {
                feature = qratio
            },
            {
                feature = wratio
            },
            {
                feature = partial_ratio
            },
            {
                feature = partial_token_set_ratio
            },
            {
                feature = partial_token_sort_ratio
            },
            {
                feature = token_set_ratio
            },
            {
                feature = token_sort_ratio
            }
        ]
    }

    svdff = {
        dump = dumps/svdff_as_00

        features = [
            {
                feature = svdff
            }
        ]
    }

    aelphy = {
        dump = dumps/aelphy_00

        features = [
            {
                feature = aelphy_f1
                train_col = f1
                test_col = f1
            },
            {
                feature = aelphy_f2
                train_col = f2
                test_col = f2
            }
        ]
    }

}


xgboost = {
    dataset = ${dataset_raw}

    dump = {
        dir = dumps2/xgboost_as_21
    }

    param = {
        num_round = 2500
        max_depth = 6
        objective = binary:logistic
        nthread = 4
        eta = 0.02
        eval_metric = [logloss, auc]
        base_score = 0.2
    }

}